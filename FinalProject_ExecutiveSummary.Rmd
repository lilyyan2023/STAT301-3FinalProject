---
title: "Stat 301-3 Final Project Executive Summary"
author: "Mingze Yan, Xi Kang"
date: "6/7/2021"
output:
  html_document: 
    code_folding: hide
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r load-packages-set-seed}
## Load Packages, Set Seed
library(tidyverse)
library(tidymodels)
library(skimr)
library(naniar)
library(kableExtra)
library(corrplot)
library(corrr)
library(tictoc)
library(themis)
set.seed(123)
```


## Introduction
This is the STAT301-3 Final Project Executive Summary.This project uses the stroke dataset from [Kaggle](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset). The goal of the project is to predict whether one person will have stroke based on their lifestyle and health status data through a classification approach. The project breaks into the Exploratory Data Analysis section and the Predictive modeling section. This report contains the key findings of each section.

## EDA important findings

```{r, load-data-and-split}
stroke_data <- readRDS("data/processed/stroke_data.rds")

## split data ----
stroke_split <- initial_split(stroke_data, prop = 0.7, strata = stroke)
stroke_train <- training(stroke_split)
stroke_test <- testing(stroke_split)
```

### Class Imbalance

The first main finding of the EDA section is that the outcome variable `stroke` has a severe class imbalance. 
```{r outcome-var-plot, fig.height= 7, fig.width=7}
# plot
stroke_train %>%
  ggplot(aes(stroke, fill = stroke)) + 
  geom_bar(show.legend = FALSE) + 
  labs(
    x = "Stroke", 
    y = "Number of Observations", 
    title = "Distribution of Response Variable"
  ) + 
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    position = position_dodge(width = 0.9), 
    vjust = -0.25
  )
```
<br>

As shown, only 174 observatinos has value `Yes` and 3403 observations are in class `No`. This observation inspired us to explore the over-sampling methods as a potential approach to solve the problem of class imbalance. In the modeling process, we compared three different recipes with two having upsampling step with different over-sampling ratios and one original recipe without oversampling.
<br>

### Skewness in Numeric Predictors

The second important finding is that variables `avg_glucose_level` and `bmi` are right-skewed. 

```{r}
# distribution of avg_glucose_level
ggplot(stroke_train, aes(avg_glucose_level)) + 
  geom_histogram(binwidth = 5) + 
  labs(
    x = "Average Glucose Level", 
    y = "number of observations", 
    title = "Distribution of `avg_glucose_level` in the Training Set"
  )

# distribution of bmi
ggplot(stroke_train, aes(bmi)) + 
  geom_histogram(binwidth = 1) + 
  labs(
    x = "BMI (Body Mass Index)", 
    y = "number of observations", 
    title = "Distribution of `bmi` in the Training Set"
  )
```
<br>


As shown in the plots, the right-skewness of both variables are pretty severe in the training set. Therefore, it leads us to log-transform `avg_glucose_level` and `bmi` in the recipe by using `step_log()`.
<br>

## Predictive Modeling Findings

```{r recipes, eval=FALSE}
# Build recipe ----
# original recipe - no over-sampling
stroke_recipe_orig <- recipe(stroke ~ ., data = stroke_train) %>% 
  step_impute_bag(bmi) %>% # bag impute bmi missing values
  step_log(avg_glucose_level, bmi) %>% 
  step_interact(bmi ~ age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# recipe with 0.2 ratio over-sampling
stroke_recipe_over_sampling_1 <- recipe(stroke ~ ., data = stroke_train) %>% 
  # over-sampling, 0.2 ratio
  step_upsample(stroke, over_ratio = 0.2) %>%
  step_impute_bag(bmi) %>% # bag impute bmi missing values
  step_log(avg_glucose_level, bmi) %>% 
  step_interact(bmi ~ age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# recipe with 0.5 ratio over-sampling
stroke_recipe_over_sampling_2 <- recipe(stroke ~ ., data = stroke_train) %>% 
  # over-sampling, 0.5 ratio
  step_upsample(stroke, over_ratio = 0.5) %>%
  step_impute_bag(bmi) %>% # bag impute bmi missing values
  step_log(avg_glucose_level, bmi) %>% 
  step_interact(bmi ~ age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```
We build the three recipes based on the EDA and then we tune the recipes to train the six models. 

1.  Elastic Net
    - parameters `mixture` and `penalty` are tuned
    
2.  Nearest neighbors
    - the number of neighbors is tuned

3.  Random forest
    - parameters `mtry` and `min_n` are tuned

4.  Boosted tree
    - parameters `mtry`, `min_n`, and `learn_rate` are tuned

5.  Support vector machine (polynomial)
    - `cost`, `degree`, and `scale_factor` are tuned

6.  Multivariate adaptive regression splines (MARS)
    - parameter `num_terms` is tuned
    
```{r load-modeling-setup-and-results}
# Load setup
load("data/stroke_setup.rda")

# load result
load("bt_tune_orig_recipe.rda")
load("en_tune_orig_recipe.rda")
load("knn_tune_orig_recipe.rda")
load("rf_tune_orig_recipe.rda")
load("svmp_tune_orig_recipe.rda")
load("mars_tune_orig_recipe.rda")
load("bt_tune_upsample_recipe1.rda")
load("en_tune_upsample_recipe1.rda")
load("knn_tune_upsample_recipe1.rda")
load("rf_tune_upsample_recipe1.rda")
load("svmp_tune_upsample_recipe1.rda")
load("mars_tune_upsample_recipe1.rda")
```

After tuning and training the models, we can find the most optimal model.

## Original Recipe 

```{r result-tibble-orig}
# result tibble
tune_results_orig <- tibble(
  model_type = c("Elastic Net", "K-nearest Neighbors", "Random Forest", "Boosted Tree", "SVM Polynomial", "Multivariate Adaptive Regression Splines"), 
  tune_info = list(en_tuned, nn_tuned, rf_tuned, bt_tuned, 
                   svmp_tuned, mars_tuned), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_orig <- tune_results_orig %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_orig %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance for 6 Candidate Models Using Original Recipe")
```
The chart shows that within this 6 candidate models using original recipe, Elastic Net model performs the best with the highest mean ROC_AUC (0.8386933) and lowest standard error (0.0065819). Because the second best performing model is boosted tree and it has a very close mean ROC_AUC (0.8382027) with elastic net model and the difference between the mean ROC_AUC of these two models is only 0.0004906 which is much smaller than the standard error of the Elastic Net model. Therefore, the Elastic Net model doesn't have a significantly better performance than the boosted tree model.

## Upsampling Recipe
```{r result-tibble-upsample1}
# result tibble
tune_results_upsample1 <- tibble(
  model_type = c("Elastic Net", "K-nearest Neighbors", "Random Forest", "Boosted Tree", "SVM Polynomial", "Multivariate Adaptive Regression Splines"), 
  tune_info = list(en_tuned_over_sampling_1, nn_tuned_over_sampling_1, rf_tuned_over_sampling_1, bt_tuned_over_sampling_1, svmp_tuned_over_sampling_1, mars_tuned_over_sampling_1), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_upsample1 <- tune_results_upsample1 %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_upsample1 %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance for 6 Candidate Models Using Upsampling Recipe")
```
The chart shows that the elastic net model is still the best performing model when using upsampling recipe. It has a highest mean ROC_AUC (0.8376819) and the lowest standard error (0.0068808). Similar to the original recipe ones, the boosted tree is the second best performing model. However, the difference between the mean ROC_AUC of these two models is 0.0045896 which is smaller than elastic net model's standard error. Therefore, it elastic net model doesn't show a significantly better performance than boosted tree model. 

## Comparison between original recipe and over-sampling recipe
By comparing the two tables above, it can be noticed that there is no strong difference in performance between original recipe and upsampling recipe. 
In order to make a more direct comparison, we choose the first and second best performing models to make a direct comparison across recipes.

```{r cross-model-cross-recipe-results}
# result tibble
tune_results_diff_recipe <- tibble(
  model_type = c("Elastic Net (Original Recipe)", 
                 "Boosted Tree (Original Recipe)", 
                 "Elastic Net (Upsampling Recipe)", 
                 "Boosted Tree (Upsampling Recipe)"), 
  tune_info = list(en_tuned, bt_tuned, 
                   en_tuned_over_sampling_1, 
                   bt_tuned_over_sampling_1), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_diff_recipe <- tune_results_diff_recipe %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_diff_recipe %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance across Different Models and Different Recipes")
```
From the table, it can be seen that the elastic net and boosted tree models using original recipe have better performance than ones using upsampling recipe. However, the difference between the mean ROC-AUC score of different recipes of the Elastic Net model is only 0.0010114, much smaller than its standard error 0.0065819. Therefore, we want to include the 0.5 upsampling ratio recipe to our discussion to make a comparison between these three recipe altogether. Therefore, the third major finding is that original recipe tends to perform better than upsampling recipe in our elastic net and boosted tree models. It might be when we over-sampling the sample whose stroke outcome is 1, these intentionally replication provides somewhat misleading input to model and leads to worse performance. 

```{r}
# load results
load("bt_tune_upsample_recipe2.rda")
load("en_tune_upsample_recipe2.rda")

# organize into table
# result tibble
tune_results_diff_recipe <- tibble(
  model_type = c("Elastic Net (Original Recipe)", 
                 "Boosted Tree (Original Recipe)", 
                 "Elastic Net (Upsampling Recipe (0.2 ratio))",
                 "Boosted Tree (Upsampling Recipe (0.2 ratio))", 
                 "Elastic Net (Upsampling Recipe (0.5 ratio))",
                 "Boosted Tree (Upsampling Recipe (0.5 ratio))"), 
  tune_info = list(en_tuned, bt_tuned, 
                   en_tuned_over_sampling_1, 
                   bt_tuned_over_sampling_1, 
                   en_tuned_over_sampling_2, 
                   bt_tuned_over_sampling_2), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_diff_recipe <- tune_results_diff_recipe %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_diff_recipe %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance across two best Models with Two Different Upsampling Recipes")
```

Based on the table above, the models with original recipe perform the best and different upsampling ratios have different performances based on models. Even though the tuning result with the oversampling step is not as good as the result without the oversampling step, the difference is very small. For the Elastic Net model the difference of the model with the 0.2 ratio upsampling recipe and the 0.5 ratio upsampling recipe compared with the original recipe is 0.0010114 and 0.003307, respectively. Both numbers are smaller than 0.0065819, the standard error of the result of the Elastic Net model with the original recipe. Similarly, in Boosted Tree model, there is no significant decline in performance when the upsampling step is applied. Also, the ROC-AUC score of the model with the upsampling recipe with over_ratio set to 0.5 (0.8337745) is actually higher than the ROC-AUC score of the model with the upsampling recipe with over_ratio set to 0.2 (0.8330923). 

As a result, the fourth major finding we can conclude is that there is no sufficient statistical evidence that applying over-sampling or setting over_ratio to a higher number will have strong negative impacts on the model performance. 

## Choose the best model and fit 
Based on our previous result, the best model is the Elastic Net model with original recipe.

```{r}
# elastic net model
show_best(en_tuned, metric = "roc_auc") %>% 
  select(-.config) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Tuning Result for Elastic Net Model")
```

By using ROC_AUC as the parameter to choose which tuning parameter values give the best performance, the table shows that elastic net model with 0.0031623 penalty and 1.00 mixture gives the best performance.

We then use the best model to fit entire training set and testing set. Similarly, we use ROC_AUC and accuracy metrics to check our performance. 

```{r fit-training-set}
en_workflow_tuned <- en_workflow %>% 
  finalize_workflow(select_best(en_tuned, metric = "roc_auc"))

en_results <- fit(en_workflow_tuned, stroke_train)
```

```{r fit-test-set}
# create metric set
stroke_metrics <- yardstick::metric_set(yardstick::roc_auc, 
                                       yardstick::accuracy)

# show result of prediction on testing data
predict(en_results, new_data = stroke_test) %>% 
  bind_cols(stroke_test %>% select(stroke)) %>% 
  bind_cols(predict(en_results, 
                    new_data = stroke_test, type = "prob")) %>%
  stroke_metrics(truth = stroke, 
                 estimate = .pred_class, .pred_Yes) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Based on the table above, the accuracy achieve 0.9510763 which is very high. However, due to the severe class imbalance in the outcome variable `stroke`, using accuracy as the judging performance metric might be misleading. In addition, ROC_AUC of the model is 0.8508733. The ROC_AUC metric provides an insight into the modelâ€™s capability at distinguishing between classes, and it is indifferent to the class imbalance. Compared with the ROC_AUC from the tuning model (0.8386933), the ROC_AUC of test model is slightly higher. This result suggests that the model performs pretty well on the new data. The fifth major finding is that when we apply our best elastic net model with original recipe to the testing dataset, the result performs better and implies that our classification model building is successful.

## Conclusion
In conclusion, in the executive summary, we summarize 5 major findings, including 2 findings from EDA of the dataset, 2 findings from comparing between original recipes and over-sampling recipes, and 1 last finding from fitting the best model to the testing dataset. 










