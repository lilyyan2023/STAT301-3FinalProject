---
title: "Stat 301-3 Final Project Executive Summary"
author: "Mingze Yan, Xi Kang"
date: "6/7/2021"
output:
  html_document: 
    code_folding: hide
    toc: true
    toc_float: true
    highlight: "tango"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r load-packages-set-seed}
## Load Packages, Set Seed
library(tidyverse)
library(tidymodels)
library(skimr)
library(naniar)
library(kableExtra)
library(corrplot)
library(corrr)
library(tictoc)
library(themis)
set.seed(123)
```


## Introduction
This is the STAT301-3 Final Project Executive Summary.This project uses the stroke dataset from [Kaggle](https://www.kaggle.com/fedesoriano/stroke-prediction-dataset). The goal of the project is to predict whether one person will have stroke based on their lifestyle and health status data through a classification approach. The project breaks into the Exploratory Data Analysis section and the Predictive modeling section. This report contains the key findings of each section.

## EDA important findings

```{r, load-data-and-split}
stroke_data <- readRDS("data/processed/stroke_data.rds")

## split data ----
stroke_split <- initial_split(stroke_data, prop = 0.7, strata = stroke)
stroke_train <- training(stroke_split)
stroke_test <- testing(stroke_split)
```

### Finding 1: Class Imbalance

The first main finding of the EDA section is that the outcome variable `stroke` has a severe class imbalance. 
```{r outcome-var-plot, fig.height= 7, fig.width=7}
# plot
stroke_train %>%
  ggplot(aes(stroke, fill = stroke)) + 
  geom_bar(show.legend = FALSE) + 
  labs(
    x = "Stroke", 
    y = "Number of Observations", 
    title = "Distribution of Response Variable"
  ) + 
  geom_text(
    aes(label = ..count..), 
    stat = "count",
    position = position_dodge(width = 0.9), 
    vjust = -0.25
  )
```
<br>

As shown, only 174 observatinos has value `Yes` and 3403 observations are in class `No`. This observation inspired us to explore the over-sampling methods as a potential approach to solve the problem of class imbalance. In the modeling process, we compared three different recipes with two having upsampling step with different over-sampling ratios and one original recipe without oversampling.
<br>

### Finding 2: Skewness in Numeric Predictors

The second important finding is that variables `avg_glucose_level` and `bmi` are right-skewed. 

```{r}
# distribution of avg_glucose_level
ggplot(stroke_train, aes(avg_glucose_level)) + 
  geom_histogram(binwidth = 5) + 
  labs(
    x = "Average Glucose Level", 
    y = "number of observations", 
    title = "Distribution of `avg_glucose_level` in the Training Set"
  )

# distribution of bmi
ggplot(stroke_train, aes(bmi)) + 
  geom_histogram(binwidth = 1) + 
  labs(
    x = "BMI (Body Mass Index)", 
    y = "number of observations", 
    title = "Distribution of `bmi` in the Training Set"
  )
```
<br>


As shown in the plots, the right-skewness of both variables are pretty severe in the training set. Therefore, it leads us to log-transform `avg_glucose_level` and `bmi` in the recipe by using `step_log()`.
<br>

## Predictive Modeling Findings

We built and trained **6 candidate model types**. The candidate models and the parameters tuned for each model is summarized below: 

1.  Elastic Net
    - parameters `mixture` and `penalty` are tuned
    
2.  Nearest neighbors
    - the number of neighbors is tuned

3.  Random forest
    - parameters `mtry` and `min_n` are tuned

4.  Boosted tree
    - parameters `mtry`, `min_n`, and `learn_rate` are tuned

5.  Support vector machine (polynomial)
    - `cost`, `degree`, and `scale_factor` are tuned

6.  Multivariate adaptive regression splines (MARS)
    - parameter `num_terms` is tuned
<br>

To investigate the impact of upsampling on the predictive capacity of the candidate models, we paired each model with two recipes: one original recipe, and one recipe with `step_upsample(stroke, over_ratio = 0.2)`, which over-samples the outcome variable with `over_ratio` of 0.2. Then, to further explore the impact of different `over_ratio` values, we paired the two best model types from the previous model tuning results with the third recipe, with the `over_ratio` parameter set to 0.5. The recipes are included in the hidden code chunk below. 
```{r recipes, eval=FALSE}
# Build recipe ----
# original recipe - no over-sampling
stroke_recipe_orig <- recipe(stroke ~ ., data = stroke_train) %>% 
  step_impute_bag(bmi) %>% # bag impute bmi missing values
  step_log(avg_glucose_level, bmi) %>% 
  step_interact(bmi ~ age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# recipe with 0.2 ratio over-sampling
stroke_recipe_over_sampling_1 <- recipe(stroke ~ ., data = stroke_train) %>% 
  # over-sampling, 0.2 ratio
  step_upsample(stroke, over_ratio = 0.2) %>%
  step_impute_bag(bmi) %>% # bag impute bmi missing values
  step_log(avg_glucose_level, bmi) %>% 
  step_interact(bmi ~ age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())

# recipe with 0.5 ratio over-sampling
stroke_recipe_over_sampling_2 <- recipe(stroke ~ ., data = stroke_train) %>% 
  # over-sampling, 0.5 ratio
  step_upsample(stroke, over_ratio = 0.5) %>%
  step_impute_bag(bmi) %>% # bag impute bmi missing values
  step_log(avg_glucose_level, bmi) %>% 
  step_interact(bmi ~ age) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_normalize(all_predictors())
```

    
```{r load-modeling-setup-and-results}
# Load setup
load("data/stroke_setup.rda")

# load result
load("bt_tune_orig_recipe.rda")
load("en_tune_orig_recipe.rda")
load("knn_tune_orig_recipe.rda")
load("rf_tune_orig_recipe.rda")
load("svmp_tune_orig_recipe.rda")
load("mars_tune_orig_recipe.rda")
load("bt_tune_upsample_recipe1.rda")
load("en_tune_upsample_recipe1.rda")
load("knn_tune_upsample_recipe1.rda")
load("rf_tune_upsample_recipe1.rda")
load("svmp_tune_upsample_recipe1.rda")
load("mars_tune_upsample_recipe1.rda")
```


Below are the key findings from the Predictive Modeling section. 

### Finding 1: Elastic Net as the Winning Model

Due to the class imbalance, the accuracy metric is misleading in conveying information regarding the model performance. Thus, we chose to find the best model based on the value of the ROC-AUC metric. 
<br>

Both the tuning result of the original recipe and the result of the first over-sampling recipe (`over_ratio = 0.2`) suggest that the best model based on the ROC-AUC metric is the elastic net model. The tuning results are explained below. 

#### Original Recipe 

```{r result-tibble-orig}
# result tibble
tune_results_orig <- tibble(
  model_type = c("Elastic Net", "K-nearest Neighbors", "Random Forest", "Boosted Tree", "SVM Polynomial", "Multivariate Adaptive Regression Splines"), 
  tune_info = list(en_tuned, nn_tuned, rf_tuned, bt_tuned, 
                   svmp_tuned, mars_tuned), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_orig <- tune_results_orig %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_orig %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance for 6 Candidate Models Using Original Recipe")
```
<br>

The chart shows that within this 6 candidate models using original recipe, Elastic Net model performs the best with the highest mean ROC-AUC (0.8386933). However, the mean ROC-AUC of the second best model (the boosted tree) differs from the mean ROC-AUC of the elastic net model by only 0.0004906, smaller than its standard error 0.0065819. Therefore, the best model of Elastic Net does not have a significantly better performance than the best model of Boosted Tree. 

#### Upsampling Recipe
```{r result-tibble-upsample1}
# result tibble
tune_results_upsample1 <- tibble(
  model_type = c("Elastic Net", "K-nearest Neighbors", "Random Forest", "Boosted Tree", "SVM Polynomial", "Multivariate Adaptive Regression Splines"), 
  tune_info = list(en_tuned_over_sampling_1, nn_tuned_over_sampling_1, rf_tuned_over_sampling_1, bt_tuned_over_sampling_1, svmp_tuned_over_sampling_1, mars_tuned_over_sampling_1), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_upsample1 <- tune_results_upsample1 %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_upsample1 %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance for 6 Candidate Models Using Upsampling Recipe")
```
<br>

The chart shows that the elastic net model is also the best performing model when using the over-sampling recipe. It has the highest mean ROC-AUC (0.8376819). In consistence with the original recipe result, the boosted tree is the second best performing model. Similarly, the difference between the mean ROC-AUC of these two models is 0.0045896, smaller than the best elastic net model's standard error. Therefore, the best elastic net model does not perform significantly better than the boosted tree model. 

### Finding 2: Impact of Over-sampling on Model Performance

By comparing the two tables above, it can be noticed that there is no significant difference in performance between the tuning results using the original recipe and the result using the upsampling recipe for the same model types. The ROC-AUC metric is not exactly the same for the same model types with different recipes. However, the difference is very small. 
<br>

For more direct comparison, we choose the first and second best performing models to compare their performance across recipes.

```{r cross-model-cross-recipe-results}
# result tibble
tune_results_diff_recipe <- tibble(
  model_type = c("Elastic Net (Original Recipe)", 
                 "Boosted Tree (Original Recipe)", 
                 "Elastic Net (Upsampling Recipe)", 
                 "Boosted Tree (Upsampling Recipe)"), 
  tune_info = list(en_tuned, bt_tuned, 
                   en_tuned_over_sampling_1, 
                   bt_tuned_over_sampling_1), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_diff_recipe <- tune_results_diff_recipe %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_diff_recipe %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance across Different Models and Different Recipes")
```
<br>

From the table, it can be seen that the Elastic Net and Boosted Tree models using original recipe have better performance than ones using the upsampling recipe. However, the difference between the mean ROC-AUC values of Elastic Net models with different recipes is only 0.0010114, much lower than the standard error 0.0065819. 
<br>

Therefore, we included the second over-sampling recipe (`over_ratio = 0.5`) to further explore the impact of over-sampling. We found that, for the Elastic Net and Boosted Tree model types, the original recipe yields better performance than the over-sampling recipes. A possible explanation from online documentation is that the over-sampling process can cause over-fitting, leading to less ideal model performance. Since we used cross-validation to let the mean ROC-AUC score reflect how well our model can generalize and predict the outcome of interest on new data, the problem caused by over-fitting is reflected by the decline in mean ROC-AUC. 

```{r}
# load results
load("bt_tune_upsample_recipe2.rda")
load("en_tune_upsample_recipe2.rda")

# organize into table
# result tibble
tune_results_diff_recipe <- tibble(
  model_type = c("Elastic Net (Original Recipe)", 
                 "Boosted Tree (Original Recipe)", 
                 "Elastic Net (Upsampling Recipe (0.2 ratio))",
                 "Boosted Tree (Upsampling Recipe (0.2 ratio))", 
                 "Elastic Net (Upsampling Recipe (0.5 ratio))",
                 "Boosted Tree (Upsampling Recipe (0.5 ratio))"), 
  tune_info = list(en_tuned, bt_tuned, 
                   en_tuned_over_sampling_1, 
                   bt_tuned_over_sampling_1, 
                   en_tuned_over_sampling_2, 
                   bt_tuned_over_sampling_2), 
  # assess model tuning results
  assessment_info = map(tune_info, collect_metrics), 
  # judge models based on the accuracty metric
  best_model = map(tune_info, ~ show_best(.x, metric = "roc_auc"))
)

# tibble for tuning performance
tune_performance_diff_recipe <- tune_results_diff_recipe %>% 
  select(model_type, best_model) %>% 
  unnest(best_model) %>% 
  group_by(model_type) %>% 
  filter(mean == max(mean)) %>%  
  arrange(desc(mean)) %>% 
  select(model_type, mean, std_err)

# print table
tune_performance_diff_recipe %>% 
  rename("ROC AUC" = mean, "Model Type" = model_type, 
         "Standard Error" = std_err) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Best Performance across two best Models with Two Different Upsampling Recipes and the Original Recipe")
```
<br>

Shown by the table above, the models with original recipe perform the best and recipes with different upsampling ratios have different performances for the same model types. Even though the tuning result for the oversampling model is not as good as the result for the original, the difference is very small. For the Elastic Net model, when comparing the result of the model with 0.2 `over_ratio` and the result of the model with 0.5 `over_ratio` with the original recipe, the differences in mean ROC-AUC are 0.0010114 and 0.003307, respectively. Both numbers are smaller than 0.0065819, the standard error of the best Elastic Net model with the original recipe. Similarly, in Boosted Tree model, there is no significant decline in performance when the upsampling step is added to the recipe, or when the `over_ratio` parameter is changed. Interestingly, for the Boosted Tree model, the mean ROC-AUC score of the model with the upsampling recipe with `over_ratio` set to 0.5 (0.8337745) is actually higher than the ROC-AUC score of the model with the upsampling recipe with over_ratio set to 0.2 (0.8330923). 

Thus, we conclude that applying over-sampling have only very little impacts on the predictive capacity of the models examined. Also, for the Elastic Net and Boosted Tree models types, setting `over_ratio` to a higher number will not have significant negative impacts on the model performance. 

### Finding 3: Best Model Shows Adequete Performance on New Data

Based on our previous result, the best model is the Elastic Net model with original recipe. The table below summarizes the parameters for the winning model. 

```{r}
# elastic net model
show_best(en_tuned, metric = "roc_auc") %>% 
  select(-.config) %>%
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>% 
  row_spec(1, color = "white",
              background = "blue") %>% 
  kableExtra::footnote(general = "Tuning Result for Elastic Net Model")
```
<br>

Using ROC-AUC as the metric for selecting the best parameter combination, the table shows that elastic net model with 0.0031623 penalty and 1.00 mixture gives the best performance.
<br>
We then fit the winning model to the test set to explore its performance on the new data. 

```{r fit-training-set}
en_workflow_tuned <- en_workflow %>% 
  finalize_workflow(select_best(en_tuned, metric = "roc_auc"))

en_results <- fit(en_workflow_tuned, stroke_train)
```

```{r fit-test-set}
# create metric set
stroke_metrics <- yardstick::metric_set(yardstick::roc_auc, 
                                       yardstick::accuracy)

# show result of prediction on testing data
predict(en_results, new_data = stroke_test) %>% 
  bind_cols(stroke_test %>% select(stroke)) %>% 
  bind_cols(predict(en_results, 
                    new_data = stroke_test, type = "prob")) %>%
  stroke_metrics(truth = stroke, 
                 estimate = .pred_class, .pred_Yes) %>% 
  kbl() %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```
<br>

As shown, the accuracy metric when fitting to the test set is 0.9510763, indicating that the proportion of correct classifications is over 95%. However, due to the severe class imbalance in the outcome variable `stroke`, it is inappropriate to evaluate model performance based on the accuracy metric. 
<br>

The ROC-AUC metric is 0.8508733, suggesting that the winning model correctly distinguishes the class of 85% of the observations of the test data. The ROC_AUC metric shows the modelâ€™s capability at distinguishing between classes, and it is not greatly impacted by the class imbalance. Compared to the ROC-AUC from the tuning model (0.8386933), the ROC-AUC metric when applying the model on the test data is slightly higher. The result suggests that the winning model shows adequate performance on the new data.

## Conclusion

In conclusion, our project builds models to predict an individual's risk of having stroke through a classification approach using the Stroke Dataset from Kaggle. 

We have 5 major findings. The 2 findings from the EDA section help us identify the nee for over-sampling and log-transformation in building the recipe. The 3 findings the Predictive Modeling section lead to the selection of the winning model, the insight into the impact of over-sampling methods on the predictive power of different models, and evaluation of the winning model's performance on the brand new data. 

Our winning model achieved ROC-AUC metric of 0.8508733, suggesting adequet performance. 

Future studies can be conducted based on the results of this project to better understand the impact of over-sampling methods on the performance of different model types and to further explore different methods for solving the problem of class imbalance.

## Github Link

[https://github.com/lilyyan2023/STAT301-3FinalProject](https://github.com/lilyyan2023/STAT301-3FinalProject){target="_blank"}








